# GPT-2 for Sentiment Analysis on Twitter Dataset

This project fine-tunes the GPT-2 model on a Twitter dataset to perform sentiment analysis. The goal is to classify tweets into different sentiment categories, such as positive, negative, or neutral. The model is evaluated using accuracy and loss metrics.

## Table of Contents
- [Project Overview](#project-overview)
- [Key Features](#key-features)
- [Dataset](#dataset)
- [Model Architecture](#model-architecture)
- [Requirements](#requirements)
- [Training](#training)
- [Evaluation](#evaluation)
- [Results](#results)
- [Usage](#usage)
- [Contributors](#contributors)
- [License](#license)

## Project Overview
This project utilizes the GPT-2 architecture for a text classification taskâ€”sentiment analysis. Tweets are used as the input data, and the model is fine-tuned to predict the sentiment of each tweet. The model is evaluated using common metrics like accuracy and evaluation loss.

## Key Features
- Fine-tunes **GPT-2** on a custom Twitter dataset.
- Predicts sentiment categories: Positive, Negative, and Neutral.
- Uses the **Hugging Face Transformers** library for model training and evaluation.
- Measures performance in terms of **accuracy** and **loss** on the evaluation dataset.

## Dataset
The dataset used for this project is a **Twitter sentiment analysis dataset**. It contains labeled tweets categorized into three sentiment types:
- **Positive**
- **Negative**
- **Neutral**

### Dataset Preprocessing:
- The dataset is tokenized using GPT-2's tokenizer.
- Special tokens are added to handle classification labels.
- The dataset is split into training and evaluation sets.

## Model Architecture
- The model architecture is based on **GPT-2** from OpenAI. Specifically, the `GPT2ForSequenceClassification` class is used, which is designed for text classification tasks.
- GPT-2 is fine-tuned using a standard supervised learning setup with a cross-entropy loss function.

## Requirements
To set up and run this project, you'll need the following dependencies:

- Python 3.x
- Transformers (`pip install transformers`)
- Datasets (`pip install datasets`)
- Evaluate (`pip install evaluate`)
- PyTorch (`pip install torch`)
- NumPy (`pip install numpy`)

## Training
The model is trained using the `Trainer` class from the Hugging Face `transformers` library. Below is an example of the training setup:

```python
from transformers import GPT2ForSequenceClassification, Trainer, TrainingArguments

# Load model and dataset
model = GPT2ForSequenceClassification.from_pretrained("gpt2", num_labels=3)
tokenized_datasets = dataset.map(tokenize_function, batched=True)

# Training arguments
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    logging_dir='./logs',
    logging_steps=10,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],
    compute_metrics=compute_metrics,
)

trainer.train()
